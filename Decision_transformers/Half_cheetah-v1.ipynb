{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##This is an Offline Decision Transformer model , where we will build our agent based on a trained data, and we wont be interacting with environment as it is costly and hard to scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'glfw'  # For MacBook, use 'glfw' instead of 'egl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sriramsohan/miniforge3/envs/Unity_Agents_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\" \n",
    "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom DataCollector for the transformers Trainer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "\n",
    "#Initializes the class with the provided dataset.\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # Automatically detects the dimensions of actions (act_dim) and observations (state_dim) from the first trajectory in the dataset.\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "\n",
    "        #Computes the total number of trajectories (n_traj) and calculates the mean and standard deviation of all states for normalization.\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "\n",
    "        #Creates a probability distribution (p_sample) for sampling trajectories based on their lengths. Longer trajectories have a higher chance of being sampled.\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "#Computes the discounted cumulative sum of rewards.\n",
    "# Used to calculate return-to-go (RTG), which is the sum of future rewards from any timestep.\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice( #Randomly samples batch_size trajectory indices (batch_inds) from the dataset\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "\n",
    "\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        #state, action, reward, dones, return-to-go, timestamp, mask= padding masks.\n",
    "        \n",
    "        #checking all  sampled trajectory indices. and selecting a starting index within a trajectory\n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            #Extracts a sequence of states starting from si up to max_len.\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "\n",
    "            #Extracts corresponding actions and rewards.\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            #Extracts dones and timesteps, adjusting timesteps to ensure they don’t exceed max_ep_len.\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        #Converts all data to PyTorch tensors for input to the Decision Transformer model.\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we need to ensure that the dictionary returns a loss, in this case we are using L-2 norm of the models action predictions and the targets\n",
    "\n",
    "\n",
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1] # Model’s predicted actions from the output.\n",
    "        action_targets = kwargs[\"actions\"] #Ground truth action labels passed via kwargs[\"actions\"].\n",
    "        attention_mask = kwargs[\"attention_mask\"] # Mask indicating valid timesteps in the sequence (1 for valid steps, 0 for padded ones).\n",
    "\n",
    "        \"\"\"he attention_mask (of shape [batch_size, max_len]) is reshaped to [batch_size * max_len].\n",
    "\t•\tThe mask is applied to retain only the valid timesteps where attention_mask > 0.\n",
    "\t•\tBoth action_preds and action_targets are filtered to include only the valid timesteps, ensuring padding does not contribute to the loss.\"\"\"\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2) ##l2. MSE\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1920' max='1920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1920/1920 23:22, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1920, training_loss=0.07056035399436951, metrics={'train_runtime': 1404.1129, 'train_samples_per_second': 85.463, 'train_steps_per_second': 1.367, 'total_flos': 147340224000000.0, 'train_loss': 0.07056035399436951, 'epoch': 120.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=120,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TrainOutput(global_step=1920, training_loss=0.07436713774998983, metrics={'train_runtime': 1433.1089, 'train_samples_per_second': 83.734, 'train_steps_per_second': 1.34, 'total_flos': 147340224000000.0, 'train_loss': 0.07436713774998983, 'epoch': 120.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the performance of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco_py\n",
    "import gym\n",
    "\n",
    "from colabgymrender.recorder import Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04489212  0.03232612  0.06034821 -0.17081618 -0.19477023 -0.05751681\n",
      "  0.0970142   0.03239178 11.0473385  -0.07997213 -0.32363245  0.3629689\n",
      "  0.42323524  0.40836537  1.1085011  -0.48743752 -0.07375081]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "os.environ['MUJOCO_GL'] = 'glfw'\n",
    "device = \"cpu\"\n",
    "\n",
    "# Move model to CPU first\n",
    "model = model.to(device)\n",
    "\n",
    "# Create environment with video recording\n",
    "env = gymnasium.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"video\",\n",
    "    episode_trigger=lambda x: True,\n",
    "    name_prefix=\"rl-video\"\n",
    ")\n",
    "\n",
    "max_ep_len = 1000\n",
    "scale = 1000.0\n",
    "TARGET_RETURN = 12000 / scale\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state, _ = env.reset()  # Unpack the tuple\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "\n",
    "\n",
    "for t in range(max_ep_len):\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    action = get_action(\n",
    "        model,\n",
    "        (states - state_mean) / state_std,\n",
    "        actions,\n",
    "        rewards,\n",
    "        target_return,\n",
    "        timesteps,\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().cpu().numpy()\n",
    "\n",
    "    state, reward, terminated, truncated, info = env.step(action)  # Updated step return values\n",
    "    done = terminated or truncated  # Combine termination conditions\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./video/rl-video-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Replace with your video path from the video directory\n",
    "video_path = \"./video/rl-video-episode-0.mp4\"  # Adjust filename as needed\n",
    "Video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "training_args.bin: 100%|██████████| 5.30k/5.30k [00:00<00:00, 36.8kB/s]\n",
      "events.out.tfevents.1737496742.Srirams-MacBook-Pro.local.2557.0: 100%|██████████| 5.99k/5.99k [00:00<00:00, 39.0kB/s]\n",
      "events.out.tfevents.1737427751.Srirams-MacBook-Pro.local.11833.0: 100%|██████████| 5.99k/5.99k [00:00<00:00, 39.7kB/s]\n",
      "model.safetensors: 100%|██████████| 5.03M/5.03M [00:01<00:00, 4.75MB/s]\n",
      "Upload 4 LFS files: 100%|██████████| 4/4 [00:01<00:00,  3.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SriramSohan/output/commit/f42ce16469bd796398d130c3d6c1df08abcce1d8', commit_message='End of training', commit_description='', oid='f42ce16469bd796398d130c3d6c1df08abcce1d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SriramSohan/output', endpoint='https://huggingface.co', repo_type='model', repo_id='SriramSohan/output'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "import torch\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.pt: 100%|██████████| 5.04M/5.04M [00:00<00:00, 11.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is pushed to the Hub. You can view your model here: https://huggingface.co/SriramSohan/Cheetah-v4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/SriramSohan/Cheetah-v4', endpoint='https://huggingface.co', repo_type='model', repo_id='SriramSohan/Cheetah-v4')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import torch\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "def push_to_hub(\n",
    "    repo_id=\"SriramSohan/halfcheetah-v4\",\n",
    "    model=None,\n",
    "    eval_env=None,\n",
    "    video_fps=30\n",
    "):\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "    api = HfApi()\n",
    "\n",
    "    # Create (or reuse) the repo on the Hub\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        local_directory = Path(tmpdirname)\n",
    "\n",
    "        # 1. Save model\n",
    "        torch.save(model.state_dict(), local_directory / \"model.pt\")\n",
    "\n",
    "        # 2. Save hyperparameters\n",
    "        hyperparameters = {\n",
    "            \"env_id\": \"HalfCheetah-v4\",\n",
    "            \"max_ep_len\": 1000,\n",
    "            \"state_dim\": 17,  # Updated for HalfCheetah\n",
    "            \"act_dim\": 6,     # Updated for HalfCheetah\n",
    "            \"target_return\": 12.0,  # Updated target return\n",
    "            \"state_mean\": state_mean.tolist(),\n",
    "            \"state_std\": state_std.tolist(),\n",
    "            \"training_args\": {\n",
    "                \"num_train_epochs\": 120,\n",
    "                \"per_device_train_batch_size\": 64,\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"weight_decay\": 1e-4,\n",
    "                \"warmup_ratio\": 0.1,\n",
    "                \"max_grad_norm\": 0.25\n",
    "            }\n",
    "        }\n",
    "        with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "            json.dump(hyperparameters, outfile)\n",
    "\n",
    "        # 3. Copy existing video (from ./video folder)\n",
    "        video_path = os.path.join(\"./video\", os.listdir(\"./video\")[0])\n",
    "        shutil.copy2(video_path, local_directory / \"replay.mp4\")\n",
    "\n",
    "        # 4. Create results.json\n",
    "        eval_data = {\n",
    "            \"env_id\": \"HalfCheetah-v4\",\n",
    "            \"eval_datetime\": datetime.datetime.now().isoformat(),\n",
    "            \"training_loss\": 0.07436713774998983,  # Your actual training loss\n",
    "            \"metrics\": {\n",
    "                \"train_runtime\": 1433.1089,\n",
    "                \"train_samples_per_second\": 83.734,\n",
    "                \"train_steps_per_second\": 1.34,\n",
    "                \"total_flos\": 147340224000000.0,\n",
    "                \"train_loss\": 0.07436713774998983,\n",
    "                \"epoch\": 120.0\n",
    "            }\n",
    "        }\n",
    "        with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "            json.dump(eval_data, outfile)\n",
    "\n",
    "        # 5. Create README.md with metadata\n",
    "        model_card = \"\"\"---\n",
    "tags:\n",
    "- HalfCheetah-v4\n",
    "- reinforcement-learning\n",
    "- decision-transformer\n",
    "- deep-reinforcement-learning\n",
    "- custom-implementation\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# Decision Transformer for HalfCheetah-v4\n",
    "\n",
    "This is a trained Decision Transformer model for the HalfCheetah-v4 environment.\n",
    "\n",
    "## Model Details\n",
    "- Environment: HalfCheetah-v4\n",
    "- Model: Decision Transformer\n",
    "- Training framework: PyTorch\n",
    "- Final Training Loss: 0.07436713774998983\n",
    "\n",
    "## Hyperparameters\n",
    "{\n",
    "\"max_ep_len\": 1000,\n",
    "\"state_dim\": 17,\n",
    "\"act_dim\": 3,\n",
    "\"target return\": 12.0,\n",
    "\"num_of_epochs\": 120,\n",
    "\"batch_size\" : 64,\n",
    "\"learning_rate\": 1e-4\n",
    "}\n",
    "The model demonstrates the running behavior learned through Decision Transformer training.\n",
    "\"\"\"\n",
    "        with open(local_directory / \"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(model_card)\n",
    "\n",
    "        # 6. Push everything to the Hub\n",
    "        api.upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=local_directory,\n",
    "            path_in_repo=\".\",  # Push all files in this temporary directory\n",
    "        )\n",
    "\n",
    "    # Print the final URL\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "push_to_hub(\n",
    "    repo_id=\"SriramSohan/Cheetah-v4\",\n",
    "    model=model,\n",
    "    eval_env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unity_Agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
